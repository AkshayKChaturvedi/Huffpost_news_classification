{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general comments:\n",
    "\n",
    "* Several Machine Learning algorithms were tried like Decision Trees, Random Forest, Gradient Boosted Trees and Logistic   Regression.\n",
    "\n",
    "\n",
    "* Out of these algorithms, logistic regression turned out to be the best in accuracy as well as time taken.\n",
    "\n",
    "\n",
    "* 'authors' and 'headline' turned out to be very useful features whereas 'date' and 'short_description' did not, from 'date' variable, features like month, day of week, day of month and year were extracted but they did not prove useful.\n",
    "\n",
    "\n",
    "* I think more and cleaner data will be required for some categories like 'ARTS', 'EDUCATION' etc. as content for different categories at times overlaps resulting in low accuracy. For example, 'New Yorker Cover Puts Trump 'In The Hole' After 'Racist' Comment', this headline does not look like it belongs to 'ARTS & CULTURE' category but according to data it does.\n",
    "\n",
    "\n",
    "* Categories which were very infrequent have been merged into one category called 'OTHER NEWS', if rows with 'OTHER NEWS' are kept, then test set accuracy is 70.91 % and if these rows are removed, then test set accuracy is 75.34 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124989 entries, 0 to 124988\n",
      "Data columns (total 6 columns):\n",
      "authors              124989 non-null object\n",
      "category             124989 non-null object\n",
      "date                 124989 non-null datetime64[ns]\n",
      "headline             124989 non-null object\n",
      "link                 124989 non-null object\n",
      "short_description    124989 non-null object\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Generic_Functions_Huffpost import model_fitting_and_get_training_accuracy, get_test_accuracy, split_data, \\\n",
    "    preprocess_comments, generate_features\n",
    "\n",
    "# Please change the path accordingly\n",
    "df = pd.read_json('C:/Users/Dell/Desktop/News_Category_Dataset.json', lines=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124989 entries, 0 to 124988\n",
      "Data columns (total 3 columns):\n",
      "authors     124989 non-null object\n",
      "category    124989 non-null object\n",
      "headline    124989 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# These variables have not been used as features so they have dropped\n",
    "df = df.drop(['date', 'link', 'short_description'], axis=1)\n",
    "\n",
    "df['authors'] = df['authors'].str.strip()\n",
    "df['category'] = df['category'].str.strip()\n",
    "df['headline'] = df['headline'].str.strip()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 124612 entries, 0 to 124988\n",
      "Data columns (total 3 columns):\n",
      "authors     124612 non-null object\n",
      "category    124612 non-null object\n",
      "headline    124612 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Many of the 'authors' variable's rows were empty, when further investigated, it turned out that these articles were cited to \n",
    "# Reuters \n",
    "df['authors'].replace('', 'Reuters', inplace=True)\n",
    "\n",
    "\n",
    "df['category'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# 'headline' variable is an important feature so if it is empty it is of no use so deleted the empty one's\n",
    "df['headline'].replace('', np.nan, inplace=True)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS          32621\n",
       "ENTERTAINMENT     14241\n",
       "HEALTHY LIVING     6686\n",
       "QUEER VOICES       4989\n",
       "BUSINESS           4246\n",
       "SPORTS             4166\n",
       "COMEDY             3962\n",
       "PARENTS            3893\n",
       "BLACK VOICES       3858\n",
       "THE WORLDPOST      3662\n",
       "WOMEN              3379\n",
       "CRIME              2890\n",
       "MEDIA              2812\n",
       "WEIRD NEWS         2670\n",
       "GREEN              2617\n",
       "IMPACT             2602\n",
       "WORLDPOST          2578\n",
       "RELIGION           2548\n",
       "STYLE              2246\n",
       "WORLD NEWS         2174\n",
       "TRAVEL             2143\n",
       "TASTE              2095\n",
       "ARTS               1509\n",
       "FIFTY              1401\n",
       "GOOD NEWS          1398\n",
       "SCIENCE            1381\n",
       "ARTS & CULTURE     1338\n",
       "TECH               1230\n",
       "COLLEGE            1144\n",
       "LATINO VOICES      1129\n",
       "EDUCATION          1004\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories with similar name or having different name but same meaning were clubbed together, foe example 'COLLEGE' and \n",
    "# 'EDUCATION' have been clubbed together into 'EDUCATION'\n",
    "df['category'] = df['category'].replace('WORLDPOST', 'THE WORLDPOST')\n",
    "df['category'] = df['category'].replace('THE WORLDPOST', 'WORLD NEWS')\n",
    "df['category'] = df['category'].replace('COLLEGE', 'EDUCATION')\n",
    "df['category'] = df['category'].replace('TECH', 'SCI & TECH')\n",
    "df['category'] = df['category'].replace('SCIENCE', 'SCI & TECH')\n",
    "df['category'] = df['category'].replace('ARTS', 'ARTS & CULTURE')\n",
    "df['category'] = df['category'].replace('QUEER VOICES', 'MINORITY VOICES')\n",
    "df['category'] = df['category'].replace('BLACK VOICES', 'MINORITY VOICES')\n",
    "df['category'] = df['category'].replace('LATINO VOICES', 'MINORITY VOICES')\n",
    "\n",
    "\n",
    "# Many categories which were very small in number and did not seem to be different from others were clubbed together \n",
    "# in 'OTHER NEWS' category\n",
    "df.loc[~df['category'].isin(['POLITICS', 'ENTERTAINMENT', 'MINORITY VOICES', 'WORLD NEWS', 'HEALTHY LIVING', 'BUSINESS', \n",
    "                             'SPORTS', 'COMEDY', 'PARENTS', 'WOMEN', 'ARTS & CULTURE', 'CRIME', 'SCI & TECH', 'RELIGION', \n",
    "                             'EDUCATION']), 'category'] = 'OTHER NEWS'\n",
    "\n",
    "\n",
    "# Please uncomment the below line to delete the rows with category 'OTHER NEWS'\n",
    "# df = df.loc[~df['category'].isin(['OTHER NEWS']), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POLITICS           32621\n",
       "OTHER NEWS         19984\n",
       "ENTERTAINMENT      14241\n",
       "MINORITY VOICES     9976\n",
       "WORLD NEWS          8414\n",
       "HEALTHY LIVING      6686\n",
       "BUSINESS            4246\n",
       "SPORTS              4166\n",
       "COMEDY              3962\n",
       "PARENTS             3893\n",
       "WOMEN               3379\n",
       "CRIME               2890\n",
       "ARTS & CULTURE      2847\n",
       "SCI & TECH          2611\n",
       "RELIGION            2548\n",
       "EDUCATION           2148\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_category = le.fit_transform(df['category'])\n",
    "\n",
    "categories = list(df['category'].unique())\n",
    "\n",
    "# combining 'authors' and 'headline' variables into a single variable so that tf-idf vectors of them can be computed together\n",
    "df_author_headline = preprocess_comments(df['authors'] + ' ' + df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = split_data(df_author_headline, df_category, test_size=0.20)\n",
    "\n",
    "train_tf_idf, tf_idf_vec, feature_names = generate_features(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions_train, lr_accuracy_train, lr_confusion_matrix_train_category, lr_classification_report_train_category, lr_classifier = model_fitting_and_get_training_accuracy(LogisticRegression, train_tf_idf, y_train, categories, random_state=0, C=0.4)\n",
    "\n",
    "lr_predictions_test, lr_accuracy_test, lr_confusion_matrix_test_category, lr_classification_report_test_category = get_test_accuracy(tf_idf_vec, x_test, lr_classifier, y_test, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy in % : 74.34421049463833\n",
      "Test set accuracy in % : 70.91441640251976\n"
     ]
    }
   ],
   "source": [
    "print('Train set accuracy in % :', lr_accuracy_train*100)\n",
    "print('Test set accuracy in % :', lr_accuracy_test*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          CRIME       0.86      0.55      0.67       595\n",
      "  ENTERTAINMENT       0.67      0.45      0.54       838\n",
      "     WORLD NEWS       0.77      0.60      0.68       795\n",
      "     OTHER NEWS       0.63      0.44      0.52       575\n",
      "       POLITICS       0.68      0.49      0.57       435\n",
      "MINORITY VOICES       0.79      0.78      0.79      2850\n",
      "          WOMEN       0.69      0.65      0.67      1363\n",
      "         COMEDY       0.81      0.64      0.72      2058\n",
      "         SPORTS       0.57      0.75      0.64      3990\n",
      "       BUSINESS       0.73      0.66      0.69       746\n",
      "     SCI & TECH       0.74      0.89      0.81      6423\n",
      "       RELIGION       0.77      0.54      0.63       504\n",
      "      EDUCATION       0.83      0.36      0.50       520\n",
      "        PARENTS       0.80      0.64      0.71       793\n",
      " ARTS & CULTURE       0.66      0.34      0.45       691\n",
      " HEALTHY LIVING       0.73      0.68      0.70      1747\n",
      "\n",
      "    avg / total       0.72      0.71      0.70     24923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lr_classification_report_test_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          CRIME       0.89      0.62      0.73      2252\n",
      "  ENTERTAINMENT       0.75      0.49      0.59      3408\n",
      "     WORLD NEWS       0.81      0.62      0.70      3167\n",
      "     OTHER NEWS       0.72      0.50      0.59      2315\n",
      "       POLITICS       0.71      0.51      0.59      1713\n",
      "MINORITY VOICES       0.80      0.81      0.80     11391\n",
      "          WOMEN       0.74      0.67      0.71      5323\n",
      "         COMEDY       0.83      0.69      0.75      7918\n",
      "         SPORTS       0.61      0.79      0.69     15994\n",
      "       BUSINESS       0.78      0.69      0.73      3147\n",
      "     SCI & TECH       0.76      0.91      0.83     26198\n",
      "       RELIGION       0.80      0.56      0.66      2044\n",
      "      EDUCATION       0.86      0.42      0.56      2091\n",
      "        PARENTS       0.85      0.68      0.75      3373\n",
      " ARTS & CULTURE       0.72      0.40      0.51      2688\n",
      " HEALTHY LIVING       0.77      0.70      0.73      6667\n",
      "\n",
      "    avg / total       0.75      0.74      0.74     99689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lr_classification_report_train_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
